{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "np.random.seed(13)\n",
    "torch.manual_seed(13)\n",
    "\n",
    "plt.ion()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The model will be trained with\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environments.SnowmanEnvironment_v2 import SnowmanEnvironment\n",
    "map_path = './Dades/andy.txt'\n",
    "save_path = './trainedModels/andy.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_rows=9\n",
    "map_cols=9\n",
    "n_actions = 4 #Modifica quan implementi gym\n",
    "\n",
    "snowman_env = SnowmanEnvironment(map_file=map_path, \n",
    "                                 n=map_rows, m=map_cols,\n",
    "                                 preprocess_mode=SnowmanEnvironment.PREPROCESS_V2,\n",
    "                                 enable_step_back_optimzation=False, \n",
    "                                 enable_blocked_snowman_optimization=True,\n",
    "                                 enable_snowball_number_optimization=True,\n",
    "                                 enable_snowball_distances_optimization=False,\n",
    "                                 enable_visited_cells_optimization=False,\n",
    "                                 enable_pushable_positions_optimization=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ReplayMemory import ReplayMemory\n",
    "from utils.ReplayMemory import Transition\n",
    "memory = ReplayMemory(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralNests.ConvNet_v1 import DQN\n",
    "policy_net = DQN(snowman_env.layers, n_actions, map_rows, map_cols).to(device)\n",
    "target_net = DQN(snowman_env.layers, n_actions, map_rows, map_cols).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainingAgents.SnowmanTrainingAgent_v2 import TrainingAgent\n",
    "\n",
    "BATCH_SIZE = 512 #mida del batch del replay\n",
    "GAMMA = 0.99 #factor de descompte de les recompeses futures\n",
    "EPS_START = 1 #valor inicial de la epsilon per la epsilon greedy policy\n",
    "EPS_END = 0.1 #valor final de la epsilon per la epsilon greedy policy\n",
    "EPS_DECAY = 10000 #controla la caiguda expoencial del valor epsilon, com més alt, mes lent disminueix\n",
    "TAU = 0.001 #controla cada quan s'actualitza la target network\n",
    "LR = 1e-3 #Learning rate de l'optimitzador\n",
    "RANDOMIZE_AGENT_POSITION_EACH_EPISODE = False\n",
    "INITIAL_RANDOM_STEPS = 0\n",
    "\n",
    "tau_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "training_agent = TrainingAgent(policy_net, target_net, optimizer, memory, EPS_END, EPS_START, EPS_DECAY, GAMMA, BATCH_SIZE, INITIAL_RANDOM_STEPS, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=200000\n",
    "jugades=70\n",
    "rewards = []\n",
    "rewards_per_jugada = []\n",
    "best_score = -100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, info = snowman_env.reset()\n",
    "    if RANDOMIZE_AGENT_POSITION_EACH_EPISODE:\n",
    "        state, info = snowman_env.randomize_agent_position()\n",
    "    \n",
    "    #Convertim l'estat a un tensor. Fem unsqueeze perquè la xarxa neuronal s'espera un batch de mapes, i state es un sol mapa.\n",
    "    #Per tant li passem un batch de un sol mapa encapsulant en un array (o tensor)\n",
    "\n",
    "    #Separem les capes de l'estat (mapa) en una capa per cada tipus de casella:\n",
    "    #state = snowman_env.split_map_layers(state)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    rewards_string = []\n",
    "    act_rewards = []\n",
    "    nJuagades = 0\n",
    "    for jugada in range(jugades):\n",
    "\n",
    "        if RANDOMIZE_AGENT_POSITION_EACH_EPISODE and jugada % 30 == 0 and False:\n",
    "            state, info = snowman_env.randomize_agent_position()\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        nJuagades = nJuagades+1\n",
    "        #Triem una accio segons la politica epsilon greedy\n",
    "        isRandom, x, y, action = training_agent.select_action_epsilon_greedy(state, snowman_env)\n",
    "        if action == None:\n",
    "            break\n",
    "\n",
    "        if debug_mode:\n",
    "            print(\"elected: \", x, y, action)\n",
    "            print(\"before moving\")\n",
    "            snowman_env.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)\n",
    "        snowman_env.move_agent_to_position(x,y)\n",
    "        if debug_mode:\n",
    "            print(\"epsilon: \",EPS_END + (EPS_START - EPS_END) * math.exp(-1. * training_agent.steps_done / EPS_DECAY))\n",
    "            print(\"after moving\")\n",
    "            snowman_env.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)\n",
    "        next_state, reward, done, _ = snowman_env.step(action)\n",
    "\n",
    "        act_rewards.append(reward)\n",
    "        rewards_per_jugada.append(reward)\n",
    "        rewards_string.append(str(reward) if isRandom else reward)\n",
    "\n",
    "        if reward >= best_score:\n",
    "            best_score = reward\n",
    "\n",
    "        #Conevrtim el reward a un tensor i el seguent state a un tensor, aixi ho guardem al replay_memory tot en tensors:\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            #next_state = torch.tensor(snowman_env.split_map_layers(next_state), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        action = torch.tensor(action, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        training_agent.replay_memory.push(state,action, next_state, reward)\n",
    "\n",
    "        #Anem al seguent estat\n",
    "        state = next_state\n",
    "\n",
    "        #OPTIMIZATION\n",
    "        training_agent.training_step()\n",
    "        \n",
    "\n",
    "\n",
    "       \n",
    "        if debug_mode:\n",
    "            print(\"epsilon: \",EPS_END + (EPS_START - EPS_END) * math.exp(-1. * training_agent.steps_done / EPS_DECAY))\n",
    "            print(\"reward\", reward.item())\n",
    "            snowman_env.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)\n",
    "            #time.sleep(1)\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "\n",
    "    #Soft update: Quan actualitzem els pesos, actualitzem el TAU % del pes enlloc de \n",
    "    #copiar totalment el pes de cada parametre per evitar variacions massives i millorar la estabilitat:\n",
    "    target_net_state_dict = training_agent.target_net.state_dict()\n",
    "    policy_net_state_dict = training_agent.policy_net.state_dict()\n",
    "\n",
    "    #per tant iterem per cada pes de la xarxa que volem actualitzar, i fem que el seu pes sigui un cert\n",
    "    # percentatge (TAU) de la xarxa objectiu més el restant (1-TAU) de la seva pròpia. Si TAU és 0.01,\n",
    "    #copia el 1% del pes de l'objectiu i conserva l'altre 99%\n",
    "    if tau_mode:\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            training_agent.target_net.load_state_dict(target_net_state_dict)\n",
    "    else:\n",
    "        if episode % 200 == 0:\n",
    "            training_agent.target_net.load_state_dict(training_agent.policy_net.state_dict())\n",
    "        \n",
    "        \n",
    "    rewards.append(sum(act_rewards))\n",
    "    if episode % 100 == 0:\n",
    "        eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * max(0,(training_agent.steps_done-training_agent.initial_random_steps)) / EPS_DECAY)\n",
    "        print(\"\\rEpisode: {}, jugades {}, total_score: {} , score: {}, best_score: {} eps: {:.3f}    \\n\".format(episode, nJuagades, sum(act_rewards), rewards_string, best_score, eps), end=\"\")\n",
    "        snowman_env.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOSTREM ELS RESULTATS (REWARDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "#save_fig(\"dqn_rewards_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards_per_jugada)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "#save_fig(\"dqn_rewards_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mapa final:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowman_env.show_map(SnowmanEnvironment.ENCODED_TEXT_MODE)\n",
    "snowman_env.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardem el model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(training_agent.policy_net.state_dict(),save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carreguem el model i inferim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inferenceEnvironment = SnowmanEnvironment(map_path, map_rows, map_cols, preprocess_mode=SnowmanEnvironment.PREPROCESS_V3)\n",
    "inferenceModel = DQN(inferenceEnvironment.layers,n_actions, map_rows, map_cols).to(device)\n",
    "inferenceModel.load_state_dict(torch.load(save_path))\n",
    "inferenceModel.eval()\n",
    "done = False\n",
    "max_steps = 100\n",
    "steps = 0\n",
    "while(not done and steps < max_steps):\n",
    "    state = inferenceEnvironment.preprocess_map(inferenceEnvironment.map)\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    valid_actions, invalid_actions = inferenceEnvironment.get_valid_actions()\n",
    "    actions = inferenceModel(state)\n",
    "    invalid_actions = torch.tensor(invalid_actions, device=device, dtype=torch.long)\n",
    "    actions[0,invalid_actions]=-100000    \n",
    "    action = actions.max(1).indices.view(1, 1)\n",
    "    _, _, done, _ = inferenceEnvironment.step(action)\n",
    "    inferenceEnvironment.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)\n",
    "    print('\\n')\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from environments.SnowmanEnvironment import SnowmanEnvironment\n",
    "snowman_env_test = SnowmanEnvironment(map_file='./Dades/test.txt', \n",
    "                                 n=9, m=12,\n",
    "                                 preprocess_mode=SnowmanEnvironment.PREPROCESS_V2,\n",
    "                                 enable_step_back_optimzation=True, \n",
    "                                 enable_blocked_snowman_optimization=True)\n",
    "print(snowman_env_test.agent_position)\n",
    "snowman_env_test.randomize_agent_position()\n",
    "snowman_env_test.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)\n",
    "print()\n",
    "print(snowman_env_test.split_map_layersV2(snowman_env_test.map)[1,:])\n",
    "print()\n",
    "print(snowman_env_test.split_map_layersV2(snowman_env_test.map)[2,:])\n",
    "#snowman_env_test.previous_agent_position = (2,6)\n",
    "#snowman_env_test.agent_position = (2,6)\n",
    "#a = snowman_env_test.adjust_reward(0,[5,4],[4,4])\n",
    "#print(snowman_env_test.agent_position)\n",
    "#print(\"****************************************************************\")\n",
    "#snowman_env_test.step(3)\n",
    "#snowman_env_test.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
