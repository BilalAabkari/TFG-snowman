{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim algunes constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class SnowmanConstants:\n",
    "    \"\"\"CONSTANTS FOR CELL IDENTIFICATION\"\"\"\n",
    "    WALL_CELL = 0\n",
    "    OUT_OFF_GRID_CELL = 0\n",
    "    SMALL_BALL_CELL = 1\n",
    "    MEDIUM_BALL_CELL = 2\n",
    "    SMALL_BALL_ON_MEDIUM_BALL_CELL= 3\n",
    "    LARGE_BALL_CELL = 4\n",
    "    SMALL_BALL_ON_LARGE_BALL_CELL = 5\n",
    "    MEDIUM_BALL_ON_LARGE_BALL_CELL = 6\n",
    "    FULL_SNOW_MAN_CELL= 7\n",
    "    GRASS_CELL = 8\n",
    "    SNOW_CELL = 9\n",
    "    CHARACTER_ON_SNOW_CELL = 10\n",
    "    CHARACTER_ON_GRASS_CELL = 11\n",
    "\n",
    "    \"\"\"CONSTANTS FOR TOKEN IDENTIFICATION\"\"\"\n",
    "    WALL_TOKEN = '#'\n",
    "    OUT_OFF_GRID_TOKEN = 'x'\n",
    "    SMALL_BALL_TOKEN = '1'\n",
    "    MEDIUM_BALL_TOKEN = '2'\n",
    "    SMALL_BALL_ON_MEDIUM_BALL_TOKEN = '3'\n",
    "    LARGE_BALL_TOKEN = '4'\n",
    "    SMALL_BALL_ON_LARGE_BALL_TOKEN = '5'\n",
    "    MEDIUM_BALL_ON_LARGE_BALL_TOKEN = '6'\n",
    "    FULL_SNOW_MAN_TOKEN= '7'\n",
    "    GRASS_TOKEN = ','\n",
    "    SNOW_TOKEN = '.'\n",
    "    CHARACTER_ON_SNOW_TOKEN = 'p'\n",
    "    CHARACTER_ON_GRASS_TOKEN = 'q'\n",
    "    CHARACTER_LEAVE_CELL = 'pq'\n",
    "\n",
    "    \"\"\"DEFINE REWARDS\"\"\"\n",
    "    error=-100\n",
    "    tonto=-50\n",
    "    cami=1\n",
    "    cim=50\n",
    "    convertir=10\n",
    "    bingo=200\n",
    "\n",
    "    actions=[\n",
    "        [[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error]],\n",
    "        [[None,None,None,error],[None,None,None,tonto],['pq',11,3,cim],[None,None,None,tonto],['pq',11,5,cim],[None,None,None,tonto],['pq',11,7,bingo],[None,None,None,tonto],['pq',11,1,cami],['pq',11,2,convertir]],\n",
    "        [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,6,cim],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,2,cami],['pq',11,4,convertir]],\n",
    "        [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,2,1,-cim],[None,2,2,-cim]],\n",
    "        [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,4,cami],['pq',11,4,cami]],\n",
    "        [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,4,1,-cim],[None,4,2,-cim]],\n",
    "        [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,4,2,-cim],[None,4,4,-cim]],   \n",
    "        [[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error]],\n",
    "        [['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,tonto],['pq',11,None,cami],['pq',11,None,cami]],\n",
    "        [['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,tonto],['pq',10,None,cami],['pq',10,None,cami]]\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_cell_codes_array():\n",
    "        return [\n",
    "            SnowmanConstants.WALL_CELL,\n",
    "            SnowmanConstants.OUT_OFF_GRID_CELL,\n",
    "            SnowmanConstants.SMALL_BALL_CELL,\n",
    "            SnowmanConstants.MEDIUM_BALL_CELL,\n",
    "            SnowmanConstants.SMALL_BALL_ON_MEDIUM_BALL_CELL,\n",
    "            SnowmanConstants.LARGE_BALL_CELL,\n",
    "            SnowmanConstants.SMALL_BALL_ON_LARGE_BALL_CELL,\n",
    "            SnowmanConstants.MEDIUM_BALL_ON_LARGE_BALL_CELL,\n",
    "            SnowmanConstants.FULL_SNOW_MAN_CELL,\n",
    "            SnowmanConstants.GRASS_CELL,\n",
    "            SnowmanConstants.SNOW_CELL,\n",
    "            SnowmanConstants.CHARACTER_ON_SNOW_CELL,\n",
    "            SnowmanConstants.CHARACTER_ON_GRASS_CELL\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tokens_array():\n",
    "        return [\n",
    "            SnowmanConstants.WALL_TOKEN,\n",
    "            SnowmanConstants.OUT_OFF_GRID_TOKEN,\n",
    "            SnowmanConstants.SMALL_BALL_TOKEN,\n",
    "            SnowmanConstants.MEDIUM_BALL_TOKEN,\n",
    "            SnowmanConstants.SMALL_BALL_ON_MEDIUM_BALL_TOKEN,\n",
    "            SnowmanConstants.LARGE_BALL_TOKEN,\n",
    "            SnowmanConstants.SMALL_BALL_ON_LARGE_BALL_TOKEN,\n",
    "            SnowmanConstants.MEDIUM_BALL_ON_LARGE_BALL_TOKEN,\n",
    "            SnowmanConstants.FULL_SNOW_MAN_TOKEN,\n",
    "            SnowmanConstants.GRASS_TOKEN,\n",
    "            SnowmanConstants.SNOW_TOKEN,\n",
    "            SnowmanConstants.CHARACTER_ON_SNOW_TOKEN,\n",
    "            SnowmanConstants.CHARACTER_ON_GRASS_TOKEN\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim l'environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class SnowmanEnvironment(gym.Env):\n",
    "\n",
    "    ENCODED_TEXT_MODE = 0\n",
    "    DECODED_TEXT_MODE = 1\n",
    "    GRAPHIC_MODE = 2 #futur, encara no implementat\n",
    "\n",
    "    def __init__(self, map_file, n, m):\n",
    "        super(SnowmanEnvironment, self).__init__()\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.map, self.decoded_map = self._read_and_encode_map(map_file, n, m)\n",
    "        self.original_map = copy.deepcopy(self.map) #Conserve the original map to allow reset\n",
    "        \n",
    "        \n",
    "        #Search the agent position\n",
    "        for i in range(n):\n",
    "            for (j) in range(m):\n",
    "                if (self.map[i,j] == SnowmanConstants.CHARACTER_ON_SNOW_CELL or \n",
    "                    self.map[i,j] == SnowmanConstants.CHARACTER_ON_GRASS_CELL):\n",
    "                    self.agent_position = (i, j)\n",
    "                    break\n",
    "        self.original_agent_position = copy.deepcopy(self.agent_position)\n",
    "\n",
    "        #Atributs necessaris per gym:\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(n, m), dtype=np.float64)\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.map = copy.deepcopy(self.original_map)\n",
    "        self.agent_position = copy.deepcopy(self.original_agent_position)\n",
    "        return self.map, { \"Agent position\" : self.agent_position}\n",
    "\n",
    "    def step(self,action): # action = 0 dreta, 1 baix, 2 esquerra, 3 dalt\n",
    "        a, b = self.agent_position\n",
    "        is_agent_on_snow = self.map[a,b] == SnowmanConstants.CHARACTER_ON_SNOW_CELL        \n",
    "        \n",
    "        inc=[[0,1,0,-1],[1,0,-1,0]]\n",
    "        next_cell=[a+inc[0][action],b+inc[1][action]] #seguent posició segons l'acció a realitzar\n",
    "        next_of_next_cell=[a+2*inc[0][action],b+2*inc[1][action]] #seguent de la seguent posició segons l'acció a realitzar\n",
    "    \n",
    "\n",
    "        mov=SnowmanConstants.actions[int(self.map[next_cell[0],next_cell[1]])][int(self.map[next_of_next_cell[0],next_of_next_cell[1]])] #busca a la matri d'acions segons el que hi ha a la posició seguent i la seguent de la seguent\n",
    "        reward=mov[3] # la tercera psosició es la recompensa\n",
    "        mov=mov[:3] # les tres primeres posicions son que hem de col.locar a la posició del jugador, la posició seguent i la posició seguent de la seguent\n",
    "        \n",
    "        for i,aux in enumerate(mov):\n",
    "            if aux!=None:\n",
    "                if aux==SnowmanConstants.CHARACTER_LEAVE_CELL:\n",
    "                    if is_agent_on_snow:\n",
    "                        f=SnowmanConstants.SNOW_CELL\n",
    "                    else:\n",
    "                        f=SnowmanConstants.GRASS_CELL\n",
    "                else:\n",
    "                    f=int(aux)\n",
    "\n",
    "                if i==0:\n",
    "                    self.map[a,b]=f\n",
    "                elif i==1:\n",
    "                    self.map[next_cell[0],next_cell[1]]=f\n",
    "                else:\n",
    "                    self.map[next_of_next_cell[0],next_of_next_cell[1]]=f\n",
    "        if reward==SnowmanConstants.bingo or reward<=SnowmanConstants.tonto:\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "\n",
    "\n",
    "        if action == 0:\n",
    "            b = b + 1\n",
    "        elif action == 1:\n",
    "            a = a + 1\n",
    "        elif action == 2:\n",
    "            b = b - 1\n",
    "        elif action == 3:\n",
    "            a = a - 1\n",
    "        if self.map[a,b] != SnowmanConstants.OUT_OFF_GRID_CELL or self.map[a,b] != SnowmanConstants.WALL_CELL:\n",
    "            self.agent_position = (a, b)\n",
    "\n",
    "\n",
    "        return self.map,reward,done, {}\n",
    "\n",
    "\n",
    "    def _read_and_encode_map(self,file,n,m):\n",
    "        tokens='x#,.qp1234567'\n",
    "        replace_tokens=[0,0,8,9,11,10,1,2,3,4,5,6,7]\n",
    "        f = open(file, \"r\")\n",
    "        decoded_map = np.empty((n, m), dtype=np.str_)\n",
    "        encoded_map = np.zeros((n,m))\n",
    "        for row,linea in enumerate(f):\n",
    "            linea=linea.rstrip('\\n\\r\\t')\n",
    "            for column,car in enumerate(linea):\n",
    "                res = tokens.find(car)\n",
    "                encoded_map[row,column]=replace_tokens[res]\n",
    "                decoded_map[row, column] = car\n",
    "                \n",
    "        f.close()\n",
    "        return encoded_map, decoded_map\n",
    "    \n",
    "    def show_map(self, mode):\n",
    "        if mode == self.ENCODED_TEXT_MODE:\n",
    "            print(self.map)\n",
    "        elif mode == self.DECODED_TEXT_MODE:\n",
    "            tokens = SnowmanConstants.get_cell_codes_array()\n",
    "            tokens_to_replace = SnowmanConstants.get_tokens_array()\n",
    "            decoded_map = copy.deepcopy(self.decoded_map)\n",
    "            for i in range(self.n):\n",
    "                for j in range(self.m):\n",
    "                    if self.map[i,j] != SnowmanConstants.WALL_CELL or self.map[i,j] != SnowmanConstants.OUT_OFF_GRID_CELL:\n",
    "                        index = tokens.index(self.map[i,j])\n",
    "                        decoded_map[i,j] = tokens_to_replace[index]\n",
    "            print(decoded_map)\n",
    "\n",
    "    def split_map_layers(self):\n",
    "        transform=[[0],[4],[5],[4,5],[6],[4,6],[5,6],[4,5,6],[2],[1],[1,3],[2,3]]\n",
    "        splitted_map=np.zeros((self.n, self.m, 7))\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.m):\n",
    "                for k in transform[int(self.map[i][j])]:\n",
    "                    splitted_map[i,j,k]=1\n",
    "        return splitted_map\n",
    "\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessitem un replay buffer que guardi les transicions (estat, accio, seguent estat, recompensa). Definim un nou tipus Transition per fer-ho:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activem el càlcul amb GPU i definim la xara DQN neuronal convolucional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be trained with cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The model will be trained with\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, n_actions, n, m):\n",
    "        super(DQN, self).__init__()\n",
    "        #conv layers params: in_channels, out_channels, kernel_size, padding=0,\n",
    "        self.conv1 = nn.Conv2d(layers, 6, 3, padding=1) \n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, padding=1)\n",
    "        self.linear1 = nn.Linear(12*n*m, 64)\n",
    "        self.linear2 = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.tanh(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.tanh(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #print(x.shape)\n",
    "        x = F.tanh(self.linear1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.softmax(self.linear2(x),  dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definim alguns hiperparametres i llavors per fer reproduible les proves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "torch.manual_seed(13)\n",
    "\n",
    "map_rows=7\n",
    "map_cols=7\n",
    "\n",
    "BATCH_SIZE = 128 #mida del batch del replay\n",
    "GAMMA = 0.99 #factor de descompte de les recompeses futures\n",
    "EPS_START = 0.9 #valor inicial de la epsilon per la epsilon greedy policy\n",
    "EPS_END = 0.05 #valor final de la epsilon per la epsilon greedy policy\n",
    "EPS_DECAY = 1000 #controla la caiguda expoencial del valor epsilon, com més alt, mes lent disminueix\n",
    "TAU = 0.005 #controla cada quan s'actualitza la target network\n",
    "LR = 1e-4 #Learning rate de l'optimitzador\n",
    "\n",
    "#Registrem l'environment al gym per poder utilitzar el mètodes de la superclase. Pot ser útil per render i algunes implementacions.\n",
    "#Per exemple el baseline pot executar algorismes amb environments personalitzats sempre que implementin alguns mètodes de gym. \n",
    "\n",
    "gym.register(\n",
    "    id='Snowman-v1.0',\n",
    "    entry_point='__main__:SnowmanEnvironment',\n",
    ")\n",
    "\n",
    "snowman_gym_env = gym.make('__main__:Snowman-v1.0', map_file='./Dades/adam.txt', n=7, m=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************DECODED MAP****************************\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  9.  9.  9.  9.  9.  0.]\n",
      " [ 0.  9.  0.  8.  0.  9.  0.]\n",
      " [ 0.  9.  1.  1.  1.  9.  0.]\n",
      " [ 0.  9.  0. 11.  0.  9.  0.]\n",
      " [ 0.  9.  9.  9.  9.  9.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "***************************ECODED MAP****************************\n",
      "[['#' '#' '#' '#' '#' '#' '#']\n",
      " ['#' '.' '.' '.' '.' '.' '#']\n",
      " ['#' '.' '#' ',' '#' '.' '#']\n",
      " ['#' '.' '1' '1' '1' '.' '#']\n",
      " ['#' '.' '#' 'q' '#' '.' '#']\n",
      " ['#' '.' '.' '.' '.' '.' '#']\n",
      " ['#' '#' '#' '#' '#' '#' '#']]\n",
      "\n",
      "\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "snowman_env = SnowmanEnvironment('./Dades/adam.txt', map_rows, map_cols)\n",
    "print(\"***************************DECODED MAP****************************\")\n",
    "snowman_env.show_map(SnowmanEnvironment.ENCODED_TEXT_MODE)\n",
    "print(\"\\n\")\n",
    "print(\"***************************ECODED MAP****************************\")\n",
    "snowman_env.show_map(SnowmanEnvironment.DECODED_TEXT_MODE)\n",
    "print(\"\\n\")\n",
    "\n",
    "n_actions =snowman_gym_env.action_space.n\n",
    "print(n_actions)\n",
    "\n",
    "state, info = snowman_gym_env.reset()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creem les dues xarxes neuronals i el replay memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class TrainingAgent:\n",
    "    def __init__(self, policy_net, target_net, replay_memory):\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.replay_memory = replay_memory\n",
    "        self.steps_done = 0\n",
    "\n",
    "    \n",
    "    def select_action_epsilon_greedy(self,state, environment):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done = self.steps_done+1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[environment.action_space.sample()]], device=device, dtype=torch.long)\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "policy_net = DQN(7, n_actions, map_rows, map_cols).to(device)\n",
    "target_net = DQN(7, n_actions, map_rows, map_cols).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "training_agent = TrainingAgent(policy_net, target_net, memory)\n",
    "\n",
    "#print(snowman_env.split_map_layers())\n",
    "a = training_agent.select_action_epsilon_greedy(torch.tensor([snowman_env.split_map_layers()], device=device, dtype=torch.float32), snowman_env)\n",
    "print(a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
