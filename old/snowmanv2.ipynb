{"cells":[{"cell_type":"markdown","metadata":{},"source":["<table align=\"left\">\n","  <td>\n","    <a ref=\"C:/Users/josep/Snowman/Reforçat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" ref=\"C:/Users/josep/Snowman/Reforçat.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n","  </td>\n","</table>\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"o2j3RSpxf_o4"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\josep\\anaconda3\\envs\\python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n","C:\\Users\\josep\\anaconda3\\envs\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n","C:\\Users\\josep\\anaconda3\\envs\\python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n","  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"]}],"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","# Common imports\n","import numpy as np\n","import os\n","import random\n","import time\n","from IPython.display import clear_output\n","\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","n=8\n","m=16"]},{"cell_type":"markdown","metadata":{},"source":["Intern code:\n","\n","- 0 : out of grid and wall (x and #)\n","- 1 : small ball\n","- 2 : medium ball\n","- 3 : small ball on top of a medium ball\n","- 4 : large ball\n","- 5 : small ball on top of a large ball\n","- 6 : medium ball on top of a large ball\n","- 7 : small ball on top of a medium ball on top of a large ball\n","- 8 : grass (,)\n","- 9 : snow (.)\n","- 10: character with snow on the floor (p)\n","- 11: character with grass on the floor (q)\n","\n","\n","Reconpenses\n","- 0 moure's sense apretar \n","- 0 moure's apretant bola petita\n","- 0 moure's apretant bola mitjana\n","- 0 moure's apretant bola grossa\n","- 100 col.locar bola mitjana sobre bola grossa\n","- 500 col.locar bola petita sobre boles mitjnes i grosses\n","- -1 Passar un instant \n","\n","Accions prohibides (-100 punts)\n","- sortir de la quadricula (trepitjar pared)\n","- fer sortir bola de la quadricula (trepitjar pared la bola)\n","- fer 2 boles grans\n","- fer dos boles mitjanes si ja tenim bola gran\n","\n","\n","Maxim episodi=50 jugades, fins acció prohibida o col.locar tres boles be"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["error=-10\n","tonto=-10\n","cami=0\n","cim=50\n","convertir=10\n","bingo=200\n","\n","actions=[\n","    [[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error]],\n","    [[None,None,None,error],[None,None,None,tonto],['pq',11,3,cim],[None,None,None,tonto],['pq',11,5,cim],[None,None,None,tonto],['pq',11,7,bingo],[None,None,None,tonto],['pq',11,1,cami],['pq',11,2,convertir]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,6,cim],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,2,cami],['pq',11,4,convertir]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,2,1,-cim],[None,2,2,-cim]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,4,cami],['pq',11,4,cami]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,4,1,-cim],[None,4,2,-cim]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,4,2,-cim],[None,4,4,-cim]],   \n","    [[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error]],\n","    [['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,tonto],['pq',11,None,cami],['pq',11,None,cami]],\n","    [['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,tonto],['pq',10,None,cami],['pq',10,None,cami]]\n","]\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["ree='x#,.qp1234567'\n","reem=[0,0,8,9,11,10,1,2,3,4,5,6,7]\n","def lleguir_tauler(nom='C:/Users/josep/Snowman/Dades/suy2.txt',n=8,m=16):\n","    f = open(nom, \"r\")\n","    tauler=np.zeros((n,m))\n","    for row,linea in enumerate(f):\n","        linea=linea.rstrip('\\n\\r\\t')\n","        for column,car in enumerate(linea):\n","            res = ree.find(car)\n","            tauler[row,column]=reem[res]\n","    f.close()\n","    return tauler\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tirCJe8Vf_pI"},"source":["# Policy Gradients"]},{"cell_type":"markdown","metadata":{"id":"wDHWW307f_pI"},"source":["To train this neural network we will need to define the target probabilities `y`. If an action is good we should increase its probability, and conversely if it is bad we should reduce it. But how do we know whether an action is good or bad? The problem is that most actions have delayed effects, so when you win or lose points in an episode, it is not clear which actions contributed to this result: was it just the last action? Or the last 10? Or just one action 50 steps earlier? This is called the _credit assignment problem_.\n","\n","The _Policy Gradients_ algorithm tackles this problem by first playing multiple episodes, then making the actions in good episodes slightly more likely, while actions in bad episodes are made slightly less likely. First we play, then we go back and think about what we did."]},{"cell_type":"markdown","metadata":{"id":"Z2EazrfDf_pJ"},"source":["Let's start by creating a function to play a single step using the model. We will also pretend for now that whatever action it takes is the right one, so we can compute the loss and its gradients (we will just save these gradients for now, and modify them later depending on how good or bad the action turned out to be):"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","enc = OneHotEncoder(sparse=False)\n","enc.fit([[0],[1],[2],[3]])\n","\n","Q=[0,0,0,0]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def step(t,action): # 0 dreta, 1 baix, 2 esquerra, 3 dalt\n","    snow=True\n","    res = np.where(t == 10)\n","    if np.size(res)==0: # and np.size(res[1])==0:\n","        snow=False\n","        res = np.where(t == 11)\n","    a=res[0]\n","    b=res[1]\n","    #print(\"a,b\",a,b,t[a][b])\n","    inc=[[0,1,0,-1],[1,0,-1,0]]\n","    seg=[a+inc[0][action],b+inc[1][action]]\n","    seg2=[a+2*inc[0][action],b+2*inc[1][action]]\n","\n","    Q[action]=Q[action]+1\n","\n","    mov=actions[int(t[seg[0],seg[1]])][int(t[seg2[0],seg2[1]])]\n","    reward=mov[3]\n","    mov=mov[:3]\n","    for i,aux in enumerate(mov):\n","        if aux!=None:\n","            if aux=='pq':\n","                if snow:\n","                    f=9\n","                else:\n","                    f=8\n","            else:\n","                f=int(aux)\n","\n","            if i==0:\n","                t[a,b]=f\n","            elif i==1:\n","                t[seg[0],seg[1]]=f\n","            else:\n","                t[seg2[0],seg2[1]]=f\n","    if reward==-77:\n","        if snow:\n","            t[a,b]=9\n","        else:\n","            t[a,b]=8\n","        a=random.randint(0,n-1)\n","        b=random.randint(0,m-1)\n","        while t[a,b]!=8 and t[a,b]!=9:\n","            a=random.randint(0,n-1)\n","            b=random.randint(0,m-1)\n","        if t[a,b]==8:\n","            t[a,b]=11\n","        else:\n","            t[a,b]=10\n","    if reward==bingo: # or reward<=tonto:\n","        done=True\n","    else:\n","        done=False\n","\n","    return (t,reward,done)\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def arreglar_taula(t):\n","    t2=t.copy()\n","    res = np.where(t2 == 11)\n","    if np.size(res)!=0:\n","        t2[res[0],res[1]]=10\n","    t2=t2/10.\n","    return t2"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"rfaXj3ihf_pJ"},"outputs":[],"source":["\n","from random import choices\n","from sklearn.preprocessing import OneHotEncoder\n","enc = OneHotEncoder(sparse=False)\n","enc.fit([[0],[1],[2],[3]])\n","\n","\n","def possible(t,act,antact,antreward):\n","    pos=True\n","    if (act==0 and antact==2) or (act==2 and antact==0) or (act==1 and antact==3) or (act==3 and antact==1):\n","        if antreward<=cami:\n","            pos=False\n","    if act==antact and  antreward<=tonto:\n","        pos=False\n","    return pos\n","\n","def play_one_step(t, model, loss_fn,antact=-3,antreward=-1):\n","    with tf.GradientTape() as tape:\n","        left_proba = tf.squeeze(model(arreglar_taula(t)[np.newaxis]))\n","        proba=left_proba\n","    \n","        \n","        r=np.random.uniform()\n","        if r>0.10:\n","            action = choices(population=range(4), k=1, weights=proba)\n","        else:\n","            action = choices(population=range(4), k=1)\n","        pop=set(range(4))\n","        while not possible(t,action[0],antact,antreward) and pop=={}:\n","            pop=pop.remove(action[0])\n","            if len(pop)>0:\n","                action = choices(population=list(sorted(list(pop))), k=1)\n","\n","        y_target = enc.transform(np.array([action])).squeeze()\n","        loss = tf.reduce_mean(loss_fn(y_target, 1-left_proba))\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    t, reward, done = step(t,action[0])\n","    return t, action[0],reward, done, grads\n","\n","#play_one_step(tauler, model, loss_fn,antact=-3,antreward=-1)"]},{"cell_type":"markdown","metadata":{"id":"VUKMlFxRf_pJ"},"source":["If `left_proba` is high, then `action` will most likely be `False` (since a random number uniformally sampled between 0 and 1 will probably not be greater than `left_proba`). And `False` means 0 when you cast it to a number, so `y_target` would be equal to 1 - 0 = 1. In other words, we set the target to 1, meaning we pretend that the probability of going left should have been 100% (so we took the right action)."]},{"cell_type":"markdown","metadata":{"id":"0rrynvHAf_pJ"},"source":["Now let's create another function that will rely on the `play_one_step()` function to play multiple episodes, returning all the rewards and gradients, for each episode and each step:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"IgF5UpXKf_pK"},"outputs":[],"source":["\n","\n","def play_multiple_episodes(n_episodes, n_max_steps, model, loss_fn,ttt):\n","    all_rewards = []\n","    all_grads = []\n","    if ttt%3==0:\n","        nom='C:/Users/josep/Snowman/Dades/suy.txt'\n","    elif ttt%3==1:\n","        nom='C:/Users/josep/Snowman/Dades/suy.txt'\n","    else:\n","        nom='C:/Users/josep/Snowman/Dades/suy.txt'\n","    \n","    tauler=lleguir_tauler(nom,n=n,m=m)\n","    maxim=-1000000\n","    for episode in range(n_episodes):\n","        t=tauler.copy()\n","        '''snow=True\n","        res = np.where(t == 10)\n","        if np.size(res[0])==0 and np.size(res[1])==0:\n","            snow=False\n","            res = np.where(t == 11)\n","        ([a],[b])=res\n","        print(res)\n","        if snow:\n","            t[a,b]=9\n","        else:\n","            t[a,b]=8\n","        a=random.randint(0,n-1)\n","        b=random.randint(0,m-1)\n","        while t[a,b]!=8 and t[a,b]!=9:\n","            a=random.randint(0,n-1)\n","            b=random.randint(0,m-1)\n","        if t[a,b]==8:\n","            t[a,b]=11\n","        else:\n","            t[a,b]=10'''\n","\n","        antact=-3\n","        reward=-1\n","        current_rewards = []\n","        current_grads = []\n","        for step in range(n_max_steps):\n","            t, antact,reward, done, grads = play_one_step(t, model, loss_fn,antact,reward)\n","            current_rewards.append(reward)\n","            current_grads.append(grads)\n","            if done:\n","                break\n","        if sum(current_rewards)>maxim:\n","            maxim=sum(current_rewards)\n","        all_rewards.append(current_rewards)\n","        all_grads.append(current_grads)\n","    return all_rewards, all_grads, maxim\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6_kT0ymaf_pK"},"source":["The Policy Gradients algorithm uses the model to play the episode several times (e.g., 10 times), then it goes back and looks at all the rewards, discounts them and normalizes them. So let's create couple functions for that: the first will compute discounted rewards; the second will normalize the discounted rewards across many episodes."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"XT_7CUgqf_pK"},"outputs":[],"source":["def discount_rewards(rewards, discount_rate):\n","    discounted = np.array(rewards)\n","    for step in range(len(rewards) - 2, -1, -1):\n","        discounted[step] += discounted[step + 1] * discount_rate\n","    return discounted\n","\n","def discount_and_normalize_rewards(all_rewards, discount_rate):\n","    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n","                              for rewards in all_rewards]\n","    flat_rewards = np.concatenate(all_discounted_rewards)\n","    reward_mean = flat_rewards.mean()\n","    reward_std = flat_rewards.std()\n","    return [(discounted_rewards - reward_mean) / reward_std\n","            for discounted_rewards in all_discounted_rewards]"]},{"cell_type":"markdown","metadata":{"id":"1R8K2lo8f_pK"},"source":["Say there were 3 actions, and after each action there was a reward: first 10, then 0, then -50. If we use a discount factor of 80%, then the 3rd action will get -50 (full credit for the last reward), but the 2nd action will only get -40 (80% credit for the last reward), and the 1st action will get 80% of -40 (-32) plus full credit for the first reward (+10), which leads to a discounted reward of -22:"]},{"cell_type":"markdown","metadata":{"id":"CNk1mWXPf_pK"},"source":["To normalize all discounted rewards across all episodes, we compute the mean and standard deviation of all the discounted rewards, and we subtract the mean from each discounted reward, and divide by the standard deviation:"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Y7KDrGQlf_pL"},"outputs":[],"source":["n_iterations = 2500\n","n_episodes_per_update = 50\n","n_max_steps = 50\n","discount_rate = 0.90"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"99FRRwF3f_pL"},"outputs":[],"source":["optimizer = keras.optimizers.Adam(learning_rate=0.001)\n","loss_fn = keras.losses.categorical_crossentropy"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"YUJMrbfKf_pL"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","model_1 (Functional)         (None, 512)               19200     \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                32832     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 4)                 260       \n","=================================================================\n","Total params: 52,292\n","Trainable params: 52,100\n","Non-trainable params: 192\n","_________________________________________________________________\n","None\n"]}],"source":["tf.keras.backend.clear_session()\n","np.random.seed(13)\n","tf.random.set_seed(13)\n","model_base = tf.keras.models.load_model('C:/Users/josep/Snowman/encoder.h5')\n","model_base.trainable=True\n","\n","model=tf.keras.models.Sequential([\n","    model_base,\n","    tf.keras.layers.Dense(64,activation='tanh'),\n","    tf.keras.layers.Dense(4,activation='softmax')\n","])\n","print(model.summary())"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def prova():\n","    tauler=lleguir_tauler('C:/Users/josep/Snowman/Dades/suy.txt',n=n,m=m)\n","    print(tauler)\n","    time.sleep(2)\n","    clear_output(wait=True)\n","    sum_rewards=0\n","\n","    for i in range(50):    \n","        action=np.argmax(model.predict(arreglar_taula(tauler)[np.newaxis,:,:]).squeeze())\n","        clear_output(wait=True)\n","        tauler, reward, done = step(tauler,action)\n","        sum_rewards+=reward\n","        print(action,i,sum_rewards)\n","        print(tauler)\n","        time.sleep(1)\n","    #clear_output(wait=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"S7eMDFhQf_pL","outputId":"517e8796-9c99-46b4-aff0-e1bbcd934edc"},"outputs":[{"name":"stdout","output_type":"stream","text":["3 49 -500\n","[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  8.  8. 11.  9.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  8.  8.  1.  8.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  8.  9.  9.  9.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  8.  9.  4.  1.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n","Iteration: 934, mean rewards: -338.0 Mov: [98787, 284267, 94989, 1854794] maxim: 240      "]}],"source":["\n","#model = tf.keras.models.load_model('snowmangradultim.h5')\n"," \n","for iteration in range(n_iterations):\n","    if (iteration+1)%100==0:\n","        prova()\n","    \n","    all_rewards, all_grads, maxim = play_multiple_episodes(n_episodes_per_update, n_max_steps, model, loss_fn,iteration)\n","\n","    total_rewards = sum([sum(ar) for ar in all_rewards])                   # Not shown in the book\n","    print(\"\\rIteration: {}, mean rewards: {:.1f} Mov: {} maxim: {}     \".format(iteration, total_rewards / n_episodes_per_update,Q,maxim), end=\"\") # Not shown\n","\n","    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n","    \n","    all_mean_grads = []\n","    for var_index in range(len(model.trainable_variables)):\n","        mean_grads = tf.reduce_mean(\n","            [final_reward * all_grads[episode_index][step][var_index]\n","             for episode_index, final_rewards in enumerate(all_final_rewards)\n","                 for step, final_reward in enumerate(final_rewards)], axis=0)\n","        all_mean_grads.append(mean_grads)\n","        \n","\n","        \n","    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Q=[0,0,0,0]\n","prova() \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDz9XPyrf_pL"},"outputs":[],"source":["print(tauler)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","model.save('snowmangradultim2.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"Copia de 18_reinforcement_learning.ipynb","provenance":[{"file_id":"https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb","timestamp":1643739375886}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}
