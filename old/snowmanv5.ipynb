{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"o2j3RSpxf_o4"},"outputs":[],"source":["import sys\n","import sklearn\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","\n","# Common imports\n","import numpy as np\n","import os\n","import random\n","import time\n","from IPython.display import clear_output\n","\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","n=8\n","m=16\n","\n","from collections import deque\n","\n","replay_memory = deque(maxlen=20000)\n"]},{"cell_type":"markdown","metadata":{},"source":["Intern code:\n","\n","- 0 : out of grid and wall (x and #)\n","- 1 : small ball\n","- 2 : medium ball\n","- 3 : small ball on top of a medium ball\n","- 4 : large ball\n","- 5 : small ball on top of a large ball\n","- 6 : medium ball on top of a large ball\n","- 7 : small ball on top of a medium ball on top of a large ball\n","- 8 : grass (,)\n","- 9 : snow (.)\n","- 10: character with snow on the floor (p) (aquest no compte a la matriu d'accions)\n","- 11: character with grass on the floor (q) (aquest no compte a la matriu d'accions)\n","\n","\n","Reconpenses\n","- 0 moure's sense apretar \n","- 0 moure's apretant bola petita\n","- 0 moure's apretant bola mitjana\n","- 0 moure's apretant bola grossa\n","- 100 col.locar bola mitjana sobre bola grossa\n","- 500 col.locar bola petita sobre boles mitjnes i grosses\n","- -1 Passar un instant \n","\n","Accions prohibides (-100 punts)\n","- sortir de la quadricula (trepitjar pared)\n","- fer sortir bola de la quadricula (trepitjar pared la bola)\n","- fer 2 boles grans\n","- fer dos boles mitjanes si ja tenim bola gran\n","\n","\n","Maxim episodi=50 jugades, fins acció prohibida o col.locar tres boles be"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["error=-100\n","tonto=-50\n","cami=1\n","cim=50\n","convertir=10\n","bingo=200\n","\n","actions=[\n","    [[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error]],\n","    [[None,None,None,error],[None,None,None,tonto],['pq',11,3,cim],[None,None,None,tonto],['pq',11,5,cim],[None,None,None,tonto],['pq',11,7,bingo],[None,None,None,tonto],['pq',11,1,cami],['pq',11,2,convertir]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,6,cim],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,2,cami],['pq',11,4,convertir]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,2,1,-cim],[None,2,2,-cim]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],['pq',11,4,cami],['pq',11,4,cami]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,4,1,-cim],[None,4,2,-cim]],\n","    [[None,None,None,error],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,None,None,tonto],[None,4,2,-cim],[None,4,4,-cim]],   \n","    [[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error],[None,None,None,error]],\n","    [['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,cami],['pq',11,None,tonto],['pq',11,None,cami],['pq',11,None,cami]],\n","    [['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,cami],['pq',10,None,tonto],['pq',10,None,cami],['pq',10,None,cami]]\n","]\n","\n","'''\n","Es tracta d'una matriu de 10x10 la primera dimensió es que hi ha a la posició seguent al jugador (home) i la segona que hi ha a la posició seguent de la seguent del jugador\n","segons l'accio que hem agafat (action = 0 dreta, 1 baix, 2 esquerra, 3 dalt) \n","Es de 10x10 ja que, el 10 i el 11 es impossible que estiguin a la posició seguent i seguent de la sefgent, ja que tant sols tenim un jugador i aquest esta en la \n","posició actual\n","La tercera dimensió de la matriu significa que cal fer amb la posició del jugador, la posició seguent del jugador, la posició següent de la seguent i la recompensa. \n","pq, significa que si el jugador actual esta sobre snow, o sigui un 10, al avancar hem de torna a possar un 10 i si estar sobre grass, o sigui un 11, al avançar hem de tornar a possar un 11\n","'''\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ree='x#,.qp1234567'\n","reem=[0,0,8,9,11,10,1,2,3,4,5,6,7]\n","def lleguir_tauler(nom='C:/Users/josep/Snowman/Dades/suy2.txt',n=8,m=16):\n","    f = open(nom, \"r\")\n","    tauler=np.zeros((n,m))\n","    for row,linea in enumerate(f):\n","        linea=linea.rstrip('\\n\\r\\t')\n","        for column,car in enumerate(linea):\n","            res = ree.find(car)\n","            tauler[row,column]=reem[res]\n","    f.close()\n","    return tauler\n","\n","n_outputs=4\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def step(t,action): # action = 0 dreta, 1 baix, 2 esquerra, 3 dalt\n","    snow=True\n","    res = np.where(t == 10) #si el jugador esta sobre snow\n","    if np.size(res)==0: \n","        snow=False\n","        res = np.where(t == 11) #si el jugador esta grass\n","    #posició del jugador t[a,b]\n","    a=res[0]\n","    b=res[1]\n","    \n","    \n","    inc=[[0,1,0,-1],[1,0,-1,0]]\n","    seg=[a+inc[0][action],b+inc[1][action]] #seguent posició segons l'acció a realitzar\n","    seg2=[a+2*inc[0][action],b+2*inc[1][action]] #seguent de la seguent posició segons l'acció a realitzar\n","\n","   \n","\n","    mov=actions[int(t[seg[0],seg[1]])][int(t[seg2[0],seg2[1]])] #busca a la matri d'acions segons el que hi ha a la posició seguent i la seguent de la seguent\n","    reward=mov[3] # la tercera psosició es la recompensa\n","    mov=mov[:3] # les tres primeres posicions son que hem de col.locar a la posició del jugador, la posició seguent i la posició seguent de la seguent\n","    \n","    for i,aux in enumerate(mov):\n","        if aux!=None:\n","            if aux=='pq':\n","                if snow:\n","                    f=9\n","                else:\n","                    f=8\n","            else:\n","                f=int(aux)\n","\n","            if i==0:\n","                t[a,b]=f\n","            elif i==1:\n","                t[seg[0],seg[1]]=f\n","            else:\n","                t[seg2[0],seg2[1]]=f\n","    if reward==bingo or reward<=tonto:\n","        done=True\n","    else:\n","        done=False\n","\n","    return t,reward,done\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def arreglar_tauler(t): # Passo cada posicio del tauler a l'interval [0,1]\n","    t2=t.copy()\n","    res = np.where(t2 == 11)\n","    if np.size(res)!=0:\n","        t2[res[0],res[1]]=10\n","    t2=t2/10.\n","    return t2\n","\n","# Aquesta funció s'ha de modificar i que doni un taula de 12 de profunditat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfaXj3ihf_pJ"},"outputs":[],"source":["\n","\n","\n","\n","def possible(t,act,antact,antreward):\n","    pos=True\n","    if (act==0 and antact==2) or (act==2 and antact==0) or (act==1 and antact==3) or (act==3 and antact==1):\n","        if antreward<=cami:\n","            pos=False\n","    if act==antact and  antreward<=tonto:\n","        pos=False\n","    return pos\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def epsilon_greedy_policy(t, epsilon=0):\n","    if np.random.rand() < epsilon:\n","        return np.random.randint(n_outputs)\n","    else:\n","        Q_values = model.predict(arreglar_tauler(t)[np.newaxis])\n","        return np.argmax(Q_values[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tf.keras.backend.clear_session()\n","np.random.seed(13)\n","tf.random.set_seed(13)\n","\n","\n","model=tf.keras.models.Sequential([\n","    tf.keras.layers.Reshape((n,m,1),input_shape=(n,m)), #Hauria de ser (n,m,12) si volem fer totes les dimensions. Treure aquest reshape\n","    tf.keras.layers.Conv2D(16,(3,3),activation='tanh',padding='same'), # col.locar a aquesta conv input_shape=(n,m,12)\n","    tf.keras.layers.Conv2D(32,(3,3),activation='tanh',padding='same'),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128,activation='tanh'),\n","    tf.keras.layers.Dense(64,activation='tanh'),\n","    tf.keras.layers.Dense(4,activation='softmax')\n","])\n","print(model.summary())\n","\n","target = keras.models.clone_model(model)\n","target.set_weights(model.get_weights())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sample_experiences(batch_size):\n","    indices = np.random.randint(len(replay_memory), size=batch_size)\n","    batch = [replay_memory[index] for index in indices]\n","    states, actions, rewards, next_states, dones = [\n","        np.array([experience[field_index] for experience in batch])\n","        for field_index in range(5)]\n","    return states, actions, rewards, next_states, dones"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def play_one_step(t, epsilon):\n","    action = epsilon_greedy_policy(t, epsilon)\n","    next_t, reward, done= step(t,tf.constant(action))\n","    replay_memory.append((t, action, reward, next_t, done))\n","    return next_t, reward, done\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch_size = 256\n","discount_rate = 0.95\n","optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n","loss_fn = keras.losses.Huber()\n","\n","def training_step(batch_size):\n","    experiences = sample_experiences(batch_size)\n","    states, actions, rewards, next_states, dones = experiences\n","    next_Q_values = model.predict(next_states)\n","    best_next_actions = np.argmax(next_Q_values, axis=1)\n","    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n","    next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)\n","    target_Q_values = (rewards + \n","                       (1 - dones) * discount_rate * next_best_Q_values)\n","    target_Q_values = target_Q_values.reshape(-1, 1)\n","    mask = tf.one_hot(actions, n_outputs)\n","    with tf.GradientTape() as tape:\n","        all_Q_values = model(states)\n","        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n","        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def prova():\n","    tauler=lleguir_tauler('C:/Users/josep/Snowman/Dades/suy.txt',n=n,m=m)\n","    print(tauler)\n","    time.sleep(2)\n","    clear_output(wait=True)\n","    sum_rewards=0\n","\n","    for i in range(50):    \n","        action=np.argmax(model.predict(arreglar_tauler(tauler)[np.newaxis,:,:]).squeeze())\n","        clear_output(wait=True)\n","        tauler, reward, done = step(tauler,action)\n","        sum_rewards+=reward\n","        print(action,i,sum_rewards)\n","        print(tauler)\n","        time.sleep(1)\n","    #clear_output(wait=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","rewards = [] \n","best_score = -100000\n","episodes=150000\n","jugades=30\n","\n","for episode in range(episodes):\n","    if episode%3==0:\n","        nom='C:/Users/josep/Snowman/Dades/suy.txt'\n","    elif episode%3==1:\n","        nom='C:/Users/josep/Snowman/Dades/suy2.txt'\n","    else:\n","        nom='C:/Users/josep/Snowman/Dades/suy3.txt'\n","    t=lleguir_tauler(nom)   \n","    epsilon = max(1 - episode / 25000, 0.01)\n","    act_rewards=[]\n","    for st in range(jugades):\n","        t, reward, done  = play_one_step(t, epsilon)\n","        act_rewards.append(reward)\n","        if done:\n","            break\n","    score=sum(act_rewards)\n","    rewards.append(score) # Not shown in the book\n","    if score >= best_score: # Not shown\n","        best_weights = model.get_weights() # Not shown\n","        best_score = score # Not shown\n","    if episode%100==0:\n","        print(\"\\rEpisode: {}, score: {}, best_score: {} eps: {:.3f} mov: {}      \".format(episode, score, best_score, epsilon, Mov), end=\"\") # Not shown\n","    if episode >= 50:\n","        training_step(batch_size)\n","        if episode % 50 == 0:\n","            target.set_weights(model.get_weights())\n","\n","model.set_weights(best_weights)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(8, 4))\n","plt.plot(rewards)\n","plt.xlabel(\"Episode\", fontsize=14)\n","plt.ylabel(\"Sum of rewards\", fontsize=14)\n","#save_fig(\"dqn_rewards_plot\")\n","plt.show()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2 49 -4798\n","[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0. 11.  8.  8.  9.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  8.  8.  1.  8.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  8.  9.  9.  9.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  8.  9.  4.  1.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n","WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}],"source":["#model.set_weights(best_weights)\n","prova()\n","model.save('dddqn1.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(best_score)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"Copia de 18_reinforcement_learning.ipynb","provenance":[{"file_id":"https://github.com/ageron/handson-ml2/blob/master/18_reinforcement_learning.ipynb","timestamp":1643739375886}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
